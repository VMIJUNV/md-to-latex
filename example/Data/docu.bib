
@article{lu_deepxde_2021,
	title = {{DeepXDE}: {A} deep learning library for solving differential equations},
	volume = {63},
	issn = {0036-1445, 1095-7200},
	shorttitle = {{DeepXDE}},
	url = {http://arxiv.org/abs/1907.04502},
	doi = {10.1137/19M1274067},
	abstract = {Deep learning has achieved remarkable success in diverse applications; however, its use in solving partial differential equations (PDEs) has emerged only recently. Here, we present an overview of physics-informed neural networks (PINNs), which embed a PDE into the loss of the neural network using automatic differentiation. The PINN algorithm is simple, and it can be applied to different types of PDEs, including integro-differential equations, fractional PDEs, and stochastic PDEs. Moreover, from the implementation point of view, PINNs solve inverse problems as easily as forward problems. We propose a new residual-based adaptive refinement (RAR) method to improve the training efficiency of PINNs. For pedagogical reasons, we compare the PINN algorithm to a standard finite element method. We also present a Python library for PINNs, DeepXDE, which is designed to serve both as an education tool to be used in the classroom as well as a research tool for solving problems in computational science and engineering. Specifically, DeepXDE can solve forward problems given initial and boundary conditions, as well as inverse problems given some extra measurements. DeepXDE supports complex-geometry domains based on the technique of constructive solid geometry, and enables the user code to be compact, resembling closely the mathematical formulation. We introduce the usage of DeepXDE and its customizability, and we also demonstrate the capability of PINNs and the user-friendliness of DeepXDE for five different examples. More broadly, DeepXDE contributes to the more rapid development of the emerging Scientific Machine Learning field.},
	number = {1},
	urldate = {2024-04-09},
	journal = {SIAM Review},
	author = {Lu, Lu and Meng, Xuhui and Mao, Zhiping and Karniadakis, George E.},
	month = jan,
	year = {2021},
	note = {arXiv:1907.04502 [physics, stat]},
	keywords = {Computer Science - Machine Learning, Physics - Computational Physics, Statistics - Machine Learning},
	pages = {208--228},
	file = {arXiv Fulltext PDF:C\:\\Users\\VMIV\\Zotero\\storage\\Y4P4A9G2\\Lu 等 - 2021 - DeepXDE A deep learning library for solving diffe.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\VMIV\\Zotero\\storage\\28WJBFWB\\1907.html:text/html},
}

@article{lu_deeponet_2021,
	title = {{DeepONet}: {Learning} nonlinear operators for identifying differential equations based on the universal approximation theorem of operators},
	volume = {3},
	issn = {2522-5839},
	shorttitle = {{DeepONet}},
	url = {http://arxiv.org/abs/1910.03193},
	doi = {10.1038/s42256-021-00302-5},
	abstract = {While it is widely known that neural networks are universal approximators of continuous functions, a less known and perhaps more powerful result is that a neural network with a single hidden layer can approximate accurately any nonlinear continuous operator. This universal approximation theorem is suggestive of the potential application of neural networks in learning nonlinear operators from data. However, the theorem guarantees only a small approximation error for a sufficient large network, and does not consider the important optimization and generalization errors. To realize this theorem in practice, we propose deep operator networks (DeepONets) to learn operators accurately and efficiently from a relatively small dataset. A DeepONet consists of two sub-networks, one for encoding the input function at a fixed number of sensors \$x\_i, i=1,{\textbackslash}dots,m\$ (branch net), and another for encoding the locations for the output functions (trunk net). We perform systematic simulations for identifying two types of operators, i.e., dynamic systems and partial differential equations, and demonstrate that DeepONet significantly reduces the generalization error compared to the fully-connected networks. We also derive theoretically the dependence of the approximation error in terms of the number of sensors (where the input function is defined) as well as the input function type, and we verify the theorem with computational results. More importantly, we observe high-order error convergence in our computational tests, namely polynomial rates (from half order to fourth order) and even exponential convergence with respect to the training dataset size.},
	number = {3},
	urldate = {2024-04-09},
	journal = {Nature Machine Intelligence},
	author = {Lu, Lu and Jin, Pengzhan and Karniadakis, George Em},
	month = mar,
	year = {2021},
	note = {arXiv:1910.03193 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	pages = {218--229},
	file = {arXiv Fulltext PDF:C\:\\Users\\VMIV\\Zotero\\storage\\TYDIB32P\\Lu 等 - 2021 - DeepONet Learning nonlinear operators for identif.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\VMIV\\Zotero\\storage\\S8V4NGTM\\1910.html:text/html},
}

@misc{li_neural_2020,
	title = {Neural {Operator}: {Graph} {Kernel} {Network} for {Partial} {Differential} {Equations}},
	shorttitle = {Neural {Operator}},
	url = {http://arxiv.org/abs/2003.03485},
	doi = {10.48550/arXiv.2003.03485},
	abstract = {The classical development of neural networks has been primarily for mappings between a finite-dimensional Euclidean space and a set of classes, or between two finite-dimensional Euclidean spaces. The purpose of this work is to generalize neural networks so that they can learn mappings between infinite-dimensional spaces (operators). The key innovation in our work is that a single set of network parameters, within a carefully designed network architecture, may be used to describe mappings between infinite-dimensional spaces and between different finite-dimensional approximations of those spaces. We formulate approximation of the infinite-dimensional mapping by composing nonlinear activation functions and a class of integral operators. The kernel integration is computed by message passing on graph networks. This approach has substantial practical consequences which we will illustrate in the context of mappings between input data to partial differential equations (PDEs) and their solutions. In this context, such learned networks can generalize among different approximation methods for the PDE (such as finite difference or finite element methods) and among approximations corresponding to different underlying levels of resolution and discretization. Experiments confirm that the proposed graph kernel network does have the desired properties and show competitive performance compared to the state of the art solvers.},
	urldate = {2024-04-09},
	publisher = {arXiv},
	author = {Li, Zongyi and Kovachki, Nikola and Azizzadenesheli, Kamyar and Liu, Burigede and Bhattacharya, Kaushik and Stuart, Andrew and Anandkumar, Anima},
	month = mar,
	year = {2020},
	note = {arXiv:2003.03485 [cs, math, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Mathematics - Numerical Analysis},
	file = {arXiv Fulltext PDF:C\:\\Users\\VMIV\\Zotero\\storage\\YQYFPAL8\\Li 等 - 2020 - Neural Operator Graph Kernel Network for Partial .pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\VMIV\\Zotero\\storage\\YFC2F3N8\\2003.html:text/html},
}

@misc{li_fourier_2021,
	title = {Fourier {Neural} {Operator} for {Parametric} {Partial} {Differential} {Equations}},
	url = {http://arxiv.org/abs/2010.08895},
	doi = {10.48550/arXiv.2010.08895},
	abstract = {The classical development of neural networks has primarily focused on learning mappings between finite-dimensional Euclidean spaces. Recently, this has been generalized to neural operators that learn mappings between function spaces. For partial differential equations (PDEs), neural operators directly learn the mapping from any functional parametric dependence to the solution. Thus, they learn an entire family of PDEs, in contrast to classical methods which solve one instance of the equation. In this work, we formulate a new neural operator by parameterizing the integral kernel directly in Fourier space, allowing for an expressive and efficient architecture. We perform experiments on Burgers' equation, Darcy flow, and Navier-Stokes equation. The Fourier neural operator is the first ML-based method to successfully model turbulent flows with zero-shot super-resolution. It is up to three orders of magnitude faster compared to traditional PDE solvers. Additionally, it achieves superior accuracy compared to previous learning-based solvers under fixed resolution.},
	urldate = {2024-04-09},
	publisher = {arXiv},
	author = {Li, Zongyi and Kovachki, Nikola and Azizzadenesheli, Kamyar and Liu, Burigede and Bhattacharya, Kaushik and Stuart, Andrew and Anandkumar, Anima},
	month = may,
	year = {2021},
	note = {arXiv:2010.08895 [cs, math]},
	keywords = {Computer Science - Machine Learning, Mathematics - Numerical Analysis},
	file = {arXiv Fulltext PDF:C\:\\Users\\VMIV\\Zotero\\storage\\9269UA85\\Li 等 - 2021 - Fourier Neural Operator for Parametric Partial Dif.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\VMIV\\Zotero\\storage\\HCK23HFJ\\2010.html:text/html},
}

@misc{peng_linear_2022,
	title = {Linear attention coupled {Fourier} neural operator for simulation of three-dimensional turbulence},
	url = {http://arxiv.org/abs/2210.04259},
	doi = {10.48550/arXiv.2210.04259},
	abstract = {Modeling three-dimensional (3D) turbulence by neural networks is difficult because 3D turbulence is highly-nonlinear with high degrees of freedom and the corresponding simulation is memory-intensive. Recently, the attention mechanism has been shown as a promising approach to boost the performance of neural networks on turbulence simulation. However, the standard self-attention mechanism uses \$O(n{\textasciicircum}2)\$ time and space with respect to input dimension \$n\$, and such quadratic complexity has become the main bottleneck for attention to be applied on 3D turbulence simulation. In this work, we resolve this issue with the concept of linear attention network. The linear attention approximates the standard attention by adding two linear projections, reducing the overall self-attention complexity from \$O(n{\textasciicircum}2)\$ to \$O(n)\$ in both time and space. The linear attention coupled Fourier neural operator (LAFNO) is developed for the simulation of 3D turbulence. Numerical simulations show that the linear attention mechanism provides 40{\textbackslash}\% error reduction at the same level of computational cost, and LAFNO can accurately reconstruct a variety of statistics and instantaneous spatial structures of 3D turbulence. The linear attention method would be helpful for the improvement of neural network models of 3D nonlinear problems involving high-dimensional data in other scientific domains.},
	urldate = {2024-04-09},
	publisher = {arXiv},
	author = {Peng, Wenhui and Yuan, Zelong and Li, Zhijie and Wang, Jianchun},
	month = nov,
	year = {2022},
	note = {arXiv:2210.04259 [physics]},
	keywords = {Physics - Fluid Dynamics},
	annote = {Comment: 36 pages, 19 figures},
	file = {arXiv Fulltext PDF:C\:\\Users\\VMIV\\Zotero\\storage\\E7T4MARI\\Peng 等 - 2022 - Linear attention coupled Fourier neural operator f.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\VMIV\\Zotero\\storage\\JXTRPPL7\\2210.html:text/html},
}

@phdthesis{__2024,
	type = {硕士学位论文},
	title = {基于深度学习求解偏微分方程的研究},
	url = {https://kns.cnki.net/KCMS/detail/detail.aspx?dbcode=CMFD&dbname=CMFD202401&filename=1023650066.nh},
	abstract = {随着人工智能和深度学习的发展,人工智能的应用已经不仅仅局限于视觉、语音、文本等传统任务,AI for Science成为了现在人工智能的重要发展方向,而人工智能求解偏微分方程是其中一个重要研究方向,有着广泛的应用场景,例如石油勘探,空气动力学研究等。在传统数学方法求解偏微分方程问题中,往往需要网格化来迭代进行求解,这需要很高的计算成本和存储成本,而且对于任意形状的区域往往需要更高的代价来得到精确的解;同时对于区域形状发生改变时,传统数学方法需要对新区域重新迭代进行求解。人工智能通过引入算子神经网络来解决上述问题,可以学习从边界形状到解函数的算子映射,解决了改变区域形状反复求解的问题,但在解决实际问题过程中仍然存在许多问题,如数据需求量大,实际采集数据或者用传统方法得到数据的成本过高等问题。为了解决上述问题本文通过引入深度算子神经网络(Deep ONet),并采取基于物理信息神经网络(PINN)的思想,将Deep ONet从数据驱动转变为物理信息约束驱动,使得神经网络在训练时不再需要每个采样点的数值解来作为约束。同时,发现在具体问题建模中,由于传统问题多数都在规则区域考虑,而在非规则区域下容易遗忘所要满足的一致性条件,本文补充了非规则区域下的一致性条件加入损失函数中。其次,在实际应用中发现在不同边界条件下的物理信息在损失函数中的数量级差距过大,这对于神经网络的训练造成了很大的困难,本文通过提出一个动态权重优化算法,使得损失函数可以收敛到更优解。然后,对于数据表示方面,传统的Deep ONet采用的是离散点来表示边界形状,本文采取了对于边界进行谱展开的表示方法,使得表示数据所需的维数降低,并且对于边界拟合的更加准确。最后,本文针对任意形状区域的求解过程提出了两种处理方法:一是将被求解区域直接嵌入至矩形区域内求解,二是通过非线性映射变换将被求解区域映射至标准矩形区域进行求解,第二种方法在数据采样时会更为方便。为了展示本文方法的实用性,本文通过对同时含有Dirichlet边界条件和Neumann边界条件以及Robin界条件和Neumann边界条件的复杂几何区域Poisson方程进行求解,并给出在水文地质学应用场景下的算例,与传统方法和Comsol得到的结果进行对比,证明了本文方法的有效性。},
	language = {zh-CN},
	urldate = {2024-04-09},
	school = {天津师范大学},
	author = {{孙靖威}},
	collaborator = {{孙华志}},
	year = {2024},
	doi = {10.27363/d.cnki.gtsfu.2023.000110},
	note = {major: 计算机科学与技术
download: 799
album: 基础科学;信息科技
CLC: O241.82;TP18
CNKICite: 0
dbcode: CMFD
dbname: CMFD202401
filename: 1023650066.nh},
	keywords = {偏微分方程, 深度学习, 神经网络, DeepONet, PINN},
	file = {Full Text PDF:C\:\\Users\\VMIV\\Zotero\\storage\\TX684IFH\\孙靖威 - 2024 - 基于深度学习求解偏微分方程的研究.pdf:application/pdf},
}

@phdthesis{__2021,
	type = {硕士学位论文},
	title = {求解偏微分方程的神经网络方法},
	url = {https://kns.cnki.net/KCMS/detail/detail.aspx?dbcode=CMFD&dbname=CMFD202102&filename=1021072465.nh},
	abstract = {偏微分方程是指未知函数及其偏导数的方程,描述自变量,未知函数及未知函数偏导数之间的关系。偏微分方程是描述客观物理世界规律最重要的数学工具之一,其在电磁学、热力学、流体力学、量子力学、几何学等学科中都有重要应用。偏微分方程的精确解难以获取,所以一般考虑获取偏微分方程的近似解。使用神经网络方法求解偏微分方程是近些年来一种新兴的偏微分方程的近似求解方法。相对传统的数值方法,大多数神经网络方法无需网格剖分,这节省了由网格剖分带来的巨大的计算开销和存储开销。此外求解偏微分方程的神经网络方法使用简单,通用性强,因此受到部分科研人员的关注。本文主要涉及求解偏微分方程的神经网络方法的相关研究。具体而言,本文的研究工作如下:1.本文探讨了使用神经网络方法求解偏微分方程的精度一致问题。该问题主要探讨在方程定义域上的误差分布情况。我们通过一个实例说明了使用神经网络方法近似求解偏微分方程存在精度不一致现象。为了缓解该现象,我们提出了区域分解-搜索奇异子域-预测(DSP)框架。本文详细地介绍了DSP框架的实现细节,并分别在Poisson方程,Helmholtz方程和Eikonal方程上完成实验。实验结果表明,我们提出的DSP框架可以很好地缓解使用神经网络方法求解偏微分方程的精度不一致现象。2.本文提出并设计了多网策略。多网策略被用来代替传统的单网策略。相对单网策略,多网策略可以显著提高算法的求解效率。在本文中,我们给出了单网策略和多网策略的定义,并通过时间复杂度分析说明了在一大类偏微分方程上,在模型复杂度接近的情况下,多网策略下的算法效率高于单网策略下的算法效率。此外,多网策略下的算法可以求解分数阶偏微分方程,但单网策略下的算法无法求解这类方程。我们分别在Burgers方程,对流扩散方程,Kdv方程,Allen-Cahn方程和四个空间分数阶偏微分方程上完成了实验,并对比了不同方法在单网策略下和多网策略下的求解精度和效率。实验结果表明,相比于单网策略下的神经网络方法,多网策略下的方法求解效率更高。并且多网策略下的神经网络方法可以求解分数阶偏微分方程。3.本文提出了一种考虑了时空间依赖性的求解偏微分方程的神经网络方法:TD-Net。TD-Net考虑通过在神经网络方法中建模时空间依赖性来提高求解偏微分方程的精度。它通过时间离散化技术建模时间依赖性,通过卷积操作建模空间依赖性。本文详细地介绍了 TD-Net的算法细节,并分析了其时间复杂度。我们分...},
	language = {zh-CN},
	urldate = {2024-04-09},
	school = {中国科学技术大学},
	author = {{王昀卓}},
	collaborator = {{陈国良} and {孙广中}},
	year = {2021},
	doi = {10.27517/d.cnki.gzkju.2021.001019},
	note = {major: 计算机软件与理论
download: 3335
album: 基础科学;信息科技
CLC: TP183;O175.2
CNKICite: 8
dbcode: CMFD
dbname: CMFD202102
filename: 1021072465.nh},
	keywords = {偏微分方程, 神经网络, 多网策略, 精度一致, 时空间依赖性},
	file = {Full Text PDF:C\:\\Users\\VMIV\\Zotero\\storage\\BNGYGKBU\\王昀卓 - 2021 - 求解偏微分方程的神经网络方法.pdf:application/pdf},
}

@misc{aldirany_multi-level_2023,
	title = {Multi-level {Neural} {Networks} for {Accurate} {Solutions} of {Boundary}-{Value} {Problems}},
	url = {http://arxiv.org/abs/2308.11503},
	doi = {10.48550/arXiv.2308.11503},
	abstract = {The solution to partial differential equations using deep learning approaches has shown promising results for several classes of initial and boundary-value problems. However, their ability to surpass, particularly in terms of accuracy, classical discretization methods such as the finite element methods, remains a significant challenge. Deep learning methods usually struggle to reliably decrease the error in their approximate solution. A new methodology to better control the error for deep learning methods is presented here. The main idea consists in computing an initial approximation to the problem using a simple neural network and in estimating, in an iterative manner, a correction by solving the problem for the residual error with a new network of increasing complexity. This sequential reduction of the residual of the partial differential equation allows one to decrease the solution error, which, in some cases, can be reduced to machine precision. The underlying explanation is that the method is able to capture at each level smaller scales of the solution using a new network. Numerical examples in 1D and 2D are presented to demonstrate the effectiveness of the proposed approach. This approach applies not only to physics informed neural networks but to other neural network solvers based on weak or strong formulations of the residual.},
	urldate = {2024-04-09},
	publisher = {arXiv},
	author = {Aldirany, Ziad and Cottereau, Régis and Laforest, Marc and Prudhomme, Serge},
	month = aug,
	year = {2023},
	note = {arXiv:2308.11503 [cs, math]},
	keywords = {Mathematics - Numerical Analysis},
	annote = {Comment: 34 pages, 20 figures},
	file = {arXiv Fulltext PDF:C\:\\Users\\VMIV\\Zotero\\storage\\AR8N3YD6\\Aldirany 等 - 2023 - Multi-level Neural Networks for Accurate Solutions.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\VMIV\\Zotero\\storage\\RFW285IC\\2308.html:text/html},
}

@article{li_long-term_2023,
	title = {Long-term predictions of turbulence by implicit {U}-{Net} enhanced {Fourier} neural operator},
	volume = {35},
	issn = {1070-6631, 1089-7666},
	url = {http://arxiv.org/abs/2305.10215},
	doi = {10.1063/5.0158830},
	abstract = {Long-term predictions of nonlinear dynamics of three-dimensional (3D) turbulence are very challenging for machine learning approaches. In this paper, we propose an implicit U-Net enhanced Fourier neural operator (IU-FNO) for stable and efficient predictions on the long-term large-scale dynamics of turbulence. The IU-FNO model employs implicit recurrent Fourier layers for deeper network extension and incorporates the U-net network for the accurate prediction on small-scale flow structures. The model is systematically tested in large-eddy simulations of three types of 3D turbulence, including forced homogeneous isotropic turbulence (HIT), temporally evolving turbulent mixing layer, and decaying homogeneous isotropic turbulence. The numerical simulations demonstrate that the IU-FNO model is more accurate than other FNO-based models including vanilla FNO, implicit FNO (IFNO) and U-Net enhanced FNO (U-FNO), and dynamic Smagorinsky model (DSM) in predicting a variety of statistics including the velocity spectrum, probability density functions (PDFs) of vorticity and velocity increments, and instantaneous spatial structures of flow field. Moreover, IU-FNO improves long-term stable predictions, which has not been achieved by the previous versions of FNO. Besides, the proposed model is much faster than traditional LES with DSM model, and can be well generalized to the situations of higher Taylor-Reynolds numbers and unseen flow regime of decaying turbulence.},
	number = {7},
	urldate = {2024-04-09},
	journal = {Physics of Fluids},
	author = {Li, Zhijie and Peng, Wenhui and Yuan, Zelong and Wang, Jianchun},
	month = jul,
	year = {2023},
	note = {arXiv:2305.10215 [math-ph, physics:nlin, physics:physics]},
	keywords = {Physics - Computational Physics, Physics - Fluid Dynamics, Mathematical Physics, Nonlinear Sciences - Chaotic Dynamics},
	pages = {075145},
	annote = {Comment: 45 pages, 21 figures},
	file = {arXiv Fulltext PDF:C\:\\Users\\VMIV\\Zotero\\storage\\JTZSFDX3\\Li 等 - 2023 - Long-term predictions of turbulence by implicit U-.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\VMIV\\Zotero\\storage\\BG8CTJ54\\2305.html:text/html},
}

@misc{guibas_adaptive_2022,
	title = {Adaptive {Fourier} {Neural} {Operators}: {Efficient} {Token} {Mixers} for {Transformers}},
	shorttitle = {Adaptive {Fourier} {Neural} {Operators}},
	url = {http://arxiv.org/abs/2111.13587},
	doi = {10.48550/arXiv.2111.13587},
	abstract = {Vision transformers have delivered tremendous success in representation learning. This is primarily due to effective token mixing through self attention. However, this scales quadratically with the number of pixels, which becomes infeasible for high-resolution inputs. To cope with this challenge, we propose Adaptive Fourier Neural Operator (AFNO) as an efficient token mixer that learns to mix in the Fourier domain. AFNO is based on a principled foundation of operator learning which allows us to frame token mixing as a continuous global convolution without any dependence on the input resolution. This principle was previously used to design FNO, which solves global convolution efficiently in the Fourier domain and has shown promise in learning challenging PDEs. To handle challenges in visual representation learning such as discontinuities in images and high resolution inputs, we propose principled architectural modifications to FNO which results in memory and computational efficiency. This includes imposing a block-diagonal structure on the channel mixing weights, adaptively sharing weights across tokens, and sparsifying the frequency modes via soft-thresholding and shrinkage. The resulting model is highly parallel with a quasi-linear complexity and has linear memory in the sequence size. AFNO outperforms self-attention mechanisms for few-shot segmentation in terms of both efficiency and accuracy. For Cityscapes segmentation with the Segformer-B3 backbone, AFNO can handle a sequence size of 65k and outperforms other efficient self-attention mechanisms.},
	urldate = {2024-04-09},
	publisher = {arXiv},
	author = {Guibas, John and Mardani, Morteza and Li, Zongyi and Tao, Andrew and Anandkumar, Anima and Catanzaro, Bryan},
	month = mar,
	year = {2022},
	note = {arXiv:2111.13587 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:C\:\\Users\\VMIV\\Zotero\\storage\\LJS2YNDG\\Guibas 等 - 2022 - Adaptive Fourier Neural Operators Efficient Token.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\VMIV\\Zotero\\storage\\FXN37NNK\\2111.html:text/html},
}

@misc{wang_learning_2021,
	title = {Learning the solution operator of parametric partial differential equations with physics-informed {DeepOnets}},
	url = {http://arxiv.org/abs/2103.10974},
	doi = {10.48550/arXiv.2103.10974},
	abstract = {Deep operator networks (DeepONets) are receiving increased attention thanks to their demonstrated capability to approximate nonlinear operators between infinite-dimensional Banach spaces. However, despite their remarkable early promise, they typically require large training data-sets consisting of paired input-output observations which may be expensive to obtain, while their predictions may not be consistent with the underlying physical principles that generated the observed data. In this work, we propose a novel model class coined as physics-informed DeepONets, which introduces an effective regularization mechanism for biasing the outputs of DeepOnet models towards ensuring physical consistency. This is accomplished by leveraging automatic differentiation to impose the underlying physical laws via soft penalty constraints during model training. We demonstrate that this simple, yet remarkably effective extension can not only yield a significant improvement in the predictive accuracy of DeepOnets, but also greatly reduce the need for large training data-sets. To this end, a remarkable observation is that physics-informed DeepONets are capable of solving parametric partial differential equations (PDEs) without any paired input-output observations, except for a set of given initial or boundary conditions. We illustrate the effectiveness of the proposed framework through a series of comprehensive numerical studies across various types of PDEs. Strikingly, a trained physics informed DeepOnet model can predict the solution of \${\textbackslash}mathcal\{O\}(10{\textasciicircum}3)\$ time-dependent PDEs in a fraction of a second -- up to three orders of magnitude faster compared a conventional PDE solver. The data and code accompanying this manuscript are publicly available at {\textbackslash}url\{https://github.com/PredictiveIntelligenceLab/Physics-informed-DeepONets\}.},
	urldate = {2024-04-09},
	publisher = {arXiv},
	author = {Wang, Sifan and Wang, Hanwen and Perdikaris, Paris},
	month = mar,
	year = {2021},
	note = {arXiv:2103.10974 [cs, math, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Mathematics - Numerical Analysis},
	annote = {Comment: 33 pages, 28 figures, 8 tables},
	file = {arXiv Fulltext PDF:C\:\\Users\\VMIV\\Zotero\\storage\\UCLPQT5B\\Wang 等 - 2021 - Learning the solution operator of parametric parti.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\VMIV\\Zotero\\storage\\ZMMKUL2C\\2103.html:text/html},
}

@misc{raissi_physics_2017,
	title = {Physics {Informed} {Deep} {Learning} ({Part} {I}): {Data}-driven {Solutions} of {Nonlinear} {Partial} {Differential} {Equations}},
	shorttitle = {Physics {Informed} {Deep} {Learning} ({Part} {I})},
	url = {http://arxiv.org/abs/1711.10561},
	doi = {10.48550/arXiv.1711.10561},
	abstract = {We introduce physics informed neural networks -- neural networks that are trained to solve supervised learning tasks while respecting any given law of physics described by general nonlinear partial differential equations. In this two part treatise, we present our developments in the context of solving two main classes of problems: data-driven solution and data-driven discovery of partial differential equations. Depending on the nature and arrangement of the available data, we devise two distinct classes of algorithms, namely continuous time and discrete time models. The resulting neural networks form a new class of data-efficient universal function approximators that naturally encode any underlying physical laws as prior information. In this first part, we demonstrate how these networks can be used to infer solutions to partial differential equations, and obtain physics-informed surrogate models that are fully differentiable with respect to all input coordinates and free parameters.},
	urldate = {2024-04-09},
	publisher = {arXiv},
	author = {Raissi, Maziar and Perdikaris, Paris and Karniadakis, George Em},
	month = nov,
	year = {2017},
	note = {arXiv:1711.10561 [cs, math, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Mathematics - Numerical Analysis, Computer Science - Artificial Intelligence, Mathematics - Dynamical Systems},
	file = {arXiv Fulltext PDF:C\:\\Users\\VMIV\\Zotero\\storage\\QSHB43YG\\Raissi 等 - 2017 - Physics Informed Deep Learning (Part I) Data-driv.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\VMIV\\Zotero\\storage\\XYJ9ZJH9\\1711.html:text/html},
}

@misc{raissi_physics_2017-1,
	title = {Physics {Informed} {Deep} {Learning} ({Part} {II}): {Data}-driven {Discovery} of {Nonlinear} {Partial} {Differential} {Equations}},
	shorttitle = {Physics {Informed} {Deep} {Learning} ({Part} {II})},
	url = {http://arxiv.org/abs/1711.10566},
	doi = {10.48550/arXiv.1711.10566},
	abstract = {We introduce physics informed neural networks -- neural networks that are trained to solve supervised learning tasks while respecting any given law of physics described by general nonlinear partial differential equations. In this second part of our two-part treatise, we focus on the problem of data-driven discovery of partial differential equations. Depending on whether the available data is scattered in space-time or arranged in fixed temporal snapshots, we introduce two main classes of algorithms, namely continuous time and discrete time models. The effectiveness of our approach is demonstrated using a wide range of benchmark problems in mathematical physics, including conservation laws, incompressible fluid flow, and the propagation of nonlinear shallow-water waves.},
	urldate = {2024-04-09},
	publisher = {arXiv},
	author = {Raissi, Maziar and Perdikaris, Paris and Karniadakis, George Em},
	month = nov,
	year = {2017},
	note = {arXiv:1711.10566 [cs, math, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Mathematics - Numerical Analysis, Computer Science - Artificial Intelligence, Mathematics - Analysis of PDEs},
	file = {arXiv Fulltext PDF:C\:\\Users\\VMIV\\Zotero\\storage\\WSMITXYH\\Raissi 等 - 2017 - Physics Informed Deep Learning (Part II) Data-dri.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\VMIV\\Zotero\\storage\\SZFPK5YG\\1711.html:text/html},
}

@misc{shi_physics-informed_2022,
	title = {Physics-informed {ConvNet}: {Learning} {Physical} {Field} from a {Shallow} {Neural} {Network}},
	shorttitle = {Physics-informed {ConvNet}},
	url = {http://arxiv.org/abs/2201.10967},
	doi = {10.48550/arXiv.2201.10967},
	abstract = {Big-data-based artificial intelligence (AI) supports profound evolution in almost all of science and technology. However, modeling and forecasting multi-physical systems remain a challenge due to unavoidable data scarcity and noise. Improving the generalization ability of neural networks by "teaching" domain knowledge and developing a new generation of models combined with the physical laws have become promising areas of machine learning research. Different from "deep" fully-connected neural networks embedded with physical information (PINN), a novel shallow framework named physics-informed convolutional network (PICN) is recommended from a CNN perspective, in which the physical field is generated by a deconvolution layer and a single convolution layer. The difference fields forming the physical operator are constructed using the pre-trained shallow convolution layer. An efficient linear interpolation network calculates the loss function involving boundary conditions and the physical constraints in irregular geometry domains. The effectiveness of the current development is illustrated through some numerical cases involving the solving (and estimation) of nonlinear physical operator equations and recovering physical information from noisy observations. Its potential advantage in approximating physical fields with multi-frequency components indicates that PICN may become an alternative neural network solver in physics-informed machine learning.},
	urldate = {2024-04-09},
	publisher = {arXiv},
	author = {Shi, Pengpeng and Zeng, Zhi and Liang, Tianshou},
	month = feb,
	year = {2022},
	note = {arXiv:2201.10967 [cs, math, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Mathematics - Numerical Analysis, 68T07, 65N99, 35Gxx,},
	file = {arXiv Fulltext PDF:C\:\\Users\\VMIV\\Zotero\\storage\\D3XLTDFK\\Shi 等 - 2022 - Physics-informed ConvNet Learning Physical Field .pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\VMIV\\Zotero\\storage\\LNI9YSXC\\2201.html:text/html},
}

@misc{markidis_old_2021,
	title = {The {Old} and the {New}: {Can} {Physics}-{Informed} {Deep}-{Learning} {Replace} {Traditional} {Linear} {Solvers}?},
	shorttitle = {The {Old} and the {New}},
	url = {http://arxiv.org/abs/2103.09655},
	doi = {10.48550/arXiv.2103.09655},
	abstract = {Physics-Informed Neural Networks (PINN) are neural networks encoding the problem governing equations, such as Partial Differential Equations (PDE), as a part of the neural network. PINNs have emerged as a new essential tool to solve various challenging problems, including computing linear systems arising from PDEs, a task for which several traditional methods exist. In this work, we focus first on evaluating the potential of PINNs as linear solvers in the case of the Poisson equation, an omnipresent equation in scientific computing. We characterize PINN linear solvers in terms of accuracy and performance under different network configurations (depth, activation functions, input data set distribution). We highlight the critical role of transfer learning. Our results show that low-frequency components of the solution converge quickly as an effect of the F-principle. In contrast, an accurate solution of the high frequencies requires an exceedingly long time. To address this limitation, we propose integrating PINNs into traditional linear solvers. We show that this integration leads to the development of new solvers whose performance is on par with other high-performance solvers, such as PETSc conjugate gradient linear solvers, in terms of performance and accuracy. Overall, while the accuracy and computational performance are still a limiting factor for the direct use of PINN linear solvers, hybrid strategies combining old traditional linear solver approaches with new emerging deep-learning techniques are among the most promising methods for developing a new class of linear solvers.},
	urldate = {2024-04-09},
	publisher = {arXiv},
	author = {Markidis, Stefano},
	month = jul,
	year = {2021},
	note = {arXiv:2103.09655 [physics]},
	keywords = {Physics - Computational Physics, Mathematics - Numerical Analysis, Computer Science - Distributed, Parallel, and Cluster Computing},
	annote = {Comment: Preprint - submitted to Frontiers},
	file = {arXiv Fulltext PDF:C\:\\Users\\VMIV\\Zotero\\storage\\RMGWL4LI\\Markidis - 2021 - The Old and the New Can Physics-Informed Deep-Lea.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\VMIV\\Zotero\\storage\\SE95Z9EI\\2103.html:text/html},
}
